{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7rZrOLcxfrP"
   },
   "source": [
    "# **Machine Learning Project**\n",
    "Hananel Mandeleyl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5rgwY55xrR3"
   },
   "source": [
    "***\n",
    "## Part 1 | Exploration\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, some importation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "h4NggVwmP4Qw",
    "outputId": "3e493e21-ce7d-4f06-f655-7b0f0f875374"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6pcscykTTap"
   },
   "source": [
    "### 1. **A sneak peak at our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tods35XtKE0S"
   },
   "source": [
    "Before any visualization and statistics, let's see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "R6kfGZw80bwl",
    "outputId": "4b0b0181-3df6-4457-8e22-98780e81a4bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_16</th>\n",
       "      <th>Feature_17</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_19</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>Feature_21</th>\n",
       "      <th>Year</th>\n",
       "      <th>Feature_23</th>\n",
       "      <th>Feature_24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.896902</td>\n",
       "      <td>6.084509</td>\n",
       "      <td>0.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>D</td>\n",
       "      <td>a21</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>13.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.693188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.632690</td>\n",
       "      <td>23.441093</td>\n",
       "      <td>6.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>N</td>\n",
       "      <td>a9</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>18.6</td>\n",
       "      <td>16.5</td>\n",
       "      <td>N</td>\n",
       "      <td>I</td>\n",
       "      <td>61.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>110.0</td>\n",
       "      <td>57.225409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.133413</td>\n",
       "      <td>5.994495</td>\n",
       "      <td>0.4</td>\n",
       "      <td>63.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>C</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.242857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.400294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.387702</td>\n",
       "      <td>18.165247</td>\n",
       "      <td>4.2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>K</td>\n",
       "      <td>a15</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>K</td>\n",
       "      <td>D</td>\n",
       "      <td>39.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>130.0</td>\n",
       "      <td>217.614788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.101356</td>\n",
       "      <td>16.652846</td>\n",
       "      <td>3.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>F</td>\n",
       "      <td>a1</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>26.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.490780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_0  Feature_1  Evaporation  Feature_3  Feature_4 Feature_5  \\\n",
       "0   0.896902   6.084509          0.6       80.0       76.0         D   \n",
       "1   2.632690  23.441093          6.4       43.0       64.0         N   \n",
       "2   1.133413   5.994495          0.4       63.0      100.0         C   \n",
       "3   2.387702  18.165247          4.2       65.0       71.0         K   \n",
       "4   2.101356  16.652846          3.2       40.0       62.0         F   \n",
       "\n",
       "  Feature_6   MaxTemp  Feature_8  Feature_9  ...  Feature_16  Feature_17  \\\n",
       "0       a21  1.107143   0.692857          5  ...        13.9        12.2   \n",
       "1        a9  1.700000   0.614286         11  ...        18.6        16.5   \n",
       "2        a4  1.242857   0.428571          6  ...        16.5         9.6   \n",
       "3       a15  1.050000   0.671429         10  ...        14.2        11.4   \n",
       "4        a1  1.950000   1.085714          3  ...        26.6        23.4   \n",
       "\n",
       "   Feature_18 Feature_19 WindGustSpeed  Feature_21  Year  Feature_23  \\\n",
       "0           D          D          28.0         7.0  2011        40.0   \n",
       "1           N          I          61.0        43.0  2012       110.0   \n",
       "2           M        NaN          15.0         7.0  2012         0.0   \n",
       "3           K          D          39.0        24.0  2010       130.0   \n",
       "4           C        NaN          30.0        20.0  2011         0.0   \n",
       "\n",
       "   Feature_24 label  \n",
       "0   10.693188     1  \n",
       "1   57.225409     0  \n",
       "2  146.400294     0  \n",
       "3  217.614788     0  \n",
       "4   81.490780     0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAGlGYAumgsG"
   },
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0nU5n_iTgAx"
   },
   "source": [
    "### 2. **Statistics and visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyLRUfeaXKA-"
   },
   "source": [
    "#### 2.1. **Shape:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tLntR5l_TcjP",
    "outputId": "1f67fc05-ad37-47c0-d84f-fd5cb5df29fc"
   },
   "outputs": [],
   "source": [
    "print(\"Rows: {}, columns: {}.\".format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80Yla3PAnTxT"
   },
   "source": [
    "As we can see, we have 22,161 entries, consisting of 25 features and 1 label column.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBC8yCSEXRLE"
   },
   "source": [
    "#### 2.2. **Feature data types:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zp2f3NSHT4OS"
   },
   "source": [
    "We would like to know:\n",
    "1. How many different data types do we have?\n",
    "2. How many columns of each type do we have?\n",
    "3. Of what type is each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "lsANqNKMXYYG",
    "outputId": "2206cc2f-f961-451e-809d-8bf904527f51"
   },
   "outputs": [],
   "source": [
    "print(\"Number of different types:\", data.dtypes.nunique(),\n",
    "      \"\\n\\nType names:\", ', '.join([str(dtype) for dtype in data.dtypes.unique()]),\n",
    "      \"\\n\\nNumber of columns of each type:\\n{}\".format(data.dtypes.value_counts().to_string()),\n",
    "      \"\\n\\nDetailed description:\\n{}\".format(data.dtypes.to_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqPpdzI1jpU0"
   },
   "source": [
    "  \\* We can see that the `object` type features are suspected to be categorical, although we have to make sure.  \n",
    "  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zc-NF4FsYD19"
   },
   "source": [
    "#### 2.3. **How many categorical features do we really have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpAaKASq8lgJ"
   },
   "source": [
    "We would like to find out which features are *truly* categorical.  \n",
    "In order to do that we will iterate through all the `object` type features and analyze each.  \n",
    "Based on the structure of the feature's entry, i.e. only consist of `char`s, or both `char`s and numbers (which we have to process differently).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xt5GArYBYgat",
    "outputId": "1af20218-abf4-4a98-af66-490ad6f8df30"
   },
   "outputs": [],
   "source": [
    "total_true_categ_cols = 0\n",
    "total_false_categ_cols = 0\n",
    "true_categ_cols_dict = {}\n",
    "chars_ints_mix_dict = {}\n",
    "\n",
    "for col in data.select_dtypes([\"object\"]):\n",
    "  col_index = data.columns.get_loc(col)\n",
    "  # we have to figure out whether the feature is a mixture of digits and chars or not\n",
    "  # to do that, we need to find a non-NaN entry in the column to examine.\n",
    "  # we iterate through entries until we find the row index of a non-NaN entry\n",
    "  i = 0\n",
    "  while pd.isna(data.iloc[i, col_index]) == True:\n",
    "    i += 1\n",
    "  # now we know for sure the i is the row index of a non-NaN entry\n",
    "  # now we check if the entry is a mixture of chars and ints or not\n",
    "  contains_digits = any(map(str.isdigit, data.iloc[i, col_index]))\n",
    "  if contains_digits:\n",
    "    chars_ints_mix_dict[col] = col_index\n",
    "    total_false_categ_cols += 1\n",
    "  else:\n",
    "    true_categ_cols_dict[col] = col_index\n",
    "    total_true_categ_cols += 1\n",
    "\n",
    "print(\"Number of true categorical features: {}\"\n",
    "      \"\\nTheir names and indices:\"\n",
    "      \"\\n{}\"\n",
    "      \"\\n\\nNumber of false categorical features: {}\"\n",
    "      \"\\nTheir names and indices:\"\n",
    "      \"\\n{}\"\n",
    "      .format(total_true_categ_cols,\n",
    "              '\\n'.join(\"{}:\\t{}\".format(feature, index) for feature, index in true_categ_cols_dict.items()),\n",
    "              total_false_categ_cols,\n",
    "              '\\n'.join(\"{}:\\t{}\".format(feature, index) for feature, index in chars_ints_mix_dict.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlVma0VEqwoD"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arjN89fFXiag"
   },
   "source": [
    "#### 2.4. **Numerical features (raw) statistics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "OtuHIM8SXlB4",
    "outputId": "d23af8c4-601c-4301-fea7-2a57b28f0131"
   },
   "outputs": [],
   "source": [
    "feature_stats = data.describe().T\n",
    "feature_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQSmgzNxq0iv"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QlcXXNFmHM4"
   },
   "source": [
    "#### 2.5. **NaN entries:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huvGBlQY12--"
   },
   "source": [
    "We would like to know how many NaN values we have for each column.  \n",
    "Furthermore, obtaining the NaN entries' percentage might be helpful as well;  \n",
    "We can use it to remove features whose NaN percentage is higher than the allowed threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "WAA2rWy7BbUm",
    "outputId": "b71d244a-3d51-4b02-ebb5-2981a9b09f23"
   },
   "outputs": [],
   "source": [
    "columns_nans_dict = {\"NaN entries\": data.isnull().sum()}\n",
    "columns_nans_df = pd.DataFrame(columns_nans_dict)\n",
    "\n",
    "columns_nans_df[\"Percentage of NaN entries\"] = columns_nans_df / data.shape[0]\n",
    "\n",
    "print(columns_nans_df.sort_values(by = ['Percentage of NaN entries'], ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNZTbAZAOYZW"
   },
   "source": [
    "Fortunately, we conclude that our data is pretty full.  \n",
    "The *Sunshine* feature is topping the list with only $\\approx$ 8% missing values.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAE6LrSHrBN6"
   },
   "source": [
    "#### 2.6. **Feature correlations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWDx3I4LrNIG"
   },
   "source": [
    "We would like to see how correlated the primal features are.  \n",
    "We'll make a correlation matrix and plot a heatmap on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "colab_type": "code",
    "id": "ddiFuHA5Qsgl",
    "outputId": "59377659-3b82-4f5b-cb9f-b62ab706ad06"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "corr_matrix = data.corr()\n",
    "mask = np.zeros_like(corr_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr_matrix, annot = True, mask = mask, square = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRLd2d4PD4sc"
   },
   "source": [
    "As we can see, some feature are highly correlated.\n",
    "My initial thought was to remove both correlated features and replace with an engineered mean feature, but, as we notice that correlation is high, we also suffer from a too high dimensionality.  \n",
    "Based on the above I've decided to leave it to our PCA/feature selector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEgz-NFCENbz"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuoS7-PCyTP2"
   },
   "source": [
    "***\n",
    "## Part 2 | Preprocessing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXk5hCKmDhQO"
   },
   "source": [
    "### Defining our configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2kAbCTGDwYD"
   },
   "source": [
    "We would like to build a configurations file/text to control the entire workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "fa-2RUV0rO6z",
    "outputId": "9eda0d2b-9205-4d04-89b4-582526842634"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# config_text = open('exp101', 'r').read()\n",
    "config_text = \"\"\"{\n",
    "                    \"nan_fill_method\": \"pad\",\n",
    "                    \"file_name\": \"first\",\n",
    "                    \"scaling_method\": \"standard\",\n",
    "                    \"pca\": {\"use\": true, \"number\": 15},\n",
    "                    \"feature_selection\": {\"use\": false, \"method\": \"f_classif\", \"number\": 10},\n",
    "                    \"models\": {\"gnb\": true, \"knn\": false, \"log_reg\": true, \"mlp\": true, \"svm\": true},\n",
    "                    \"mode\": \"test\"\n",
    "                  }\"\"\"\n",
    "config_dict = json.loads(config_text)\n",
    "\n",
    "class Config:\n",
    "  def __init__(self, config_dict):\n",
    "    try:\n",
    "      self.target_file = config_dict.get('file_name', 'def_file')\n",
    "      if self.target_file is 'def_file':\n",
    "        print('Warning! No file name, saving to default.')\n",
    "\n",
    "      self.nan_fill_method = config_dict['nan_fill_method']\n",
    "\n",
    "      self.scaling_method = config_dict['scaling_method']\n",
    "\n",
    "      self.use_pca = config_dict['pca'].get('use', False)\n",
    "      if self.use_pca:\n",
    "        self.pca_number_of_componnents = config_dict['pca']['number']\n",
    "\n",
    "      self.use_feature_select = config_dict['feature_selection'].get('use', False)\n",
    "      if self.use_feature_select:\n",
    "        self.fs_method = config_dict['feature_selection']['method']\n",
    "        self.fs_number_of_features = config_dict['feature_selection']['number']\n",
    "      \n",
    "      self.use_gnb = config_dict['models']['gnb']\n",
    "      self.use_knn = config_dict['models']['knn']\n",
    "      self.use_log_reg = config_dict['models']['log_reg']\n",
    "      self.use_mlp = config_dict['models']['mlp']\n",
    "      self.use_svm = config_dict['models']['svm']\n",
    "      \n",
    "      self.mode = config_dict['mode']\n",
    "    except ValueError as e:\n",
    "      print('Missing required value in config file.')\n",
    "      raise e\n",
    "\n",
    "config = Config(config_dict)\n",
    "print(\"Selected configuration:\\n\",\n",
    "      \"\\nNaN filling method:\", config.nan_fill_method,\n",
    "      \"\\nScaling method:\", config.scaling_method,\n",
    "      \"\\nUse PCA:\", config.use_pca,\n",
    "      \"\\nPCA number of components: {}\".format(config.pca_number_of_componnents) if config.use_pca else \"\",\n",
    "      \"\\nUse feature selection:\", config.use_feature_select,\n",
    "      \"\\Feature selection method: {}, number of features: {}\".format(config.fs_method, config.fs_number_of_features) if config.use_feature_select else \"\",\n",
    "      \"\\nUse Gaussian Naive Bayes:\", config.use_gnb,\n",
    "      \"\\nUse KNN:\", config.use_knn,\n",
    "      \"\\nUse Logistic Regression:\", config.use_log_reg,\n",
    "      \"\\nUse MLP:\", config.use_mlp,\n",
    "      \"\\nUse SVM:\", config.use_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Wj78WczDua_"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JAMpBDNokBA"
   },
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lUnxCVwo85A"
   },
   "outputs": [],
   "source": [
    "def is_int_or_dot(some_char):\n",
    "  if some_char.isdigit() or some_char == '.':\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def remove_chars(some_string):\n",
    "  # We filter out every char that isn't a digit or a '.'\n",
    "  some_string_wo_chars = ''.join(filter(is_int_or_dot, some_string))\n",
    "  # If the entry doesn't contain any digits, e.g. 'nanmm', we return NaN\n",
    "  if len(some_string_wo_chars) != 0:\n",
    "    return float(some_string_wo_chars)\n",
    "  else:\n",
    "    return np.nan\n",
    "\n",
    "def remove_chars_from_col(df, column):\n",
    "  column_index = data.columns.get_loc(column)\n",
    "  # We apply remove_chars() on every entry that isn't already NaN\n",
    "  df[column] = [remove_chars(entry) if pd.isna(entry) == False else np.nan for entry in df[column]]\n",
    "\n",
    "def categ_col_to_num(df, column):\n",
    "  # We change column's data type to 'category' in order to use pandas.DataFrame.cat.codes\n",
    "  df[column] = df[column].astype('category')\n",
    "  # We convert categorical values to numerical values using pandas.DataFrame.cat.codes\n",
    "  # Note: pandas.DataFrame.cat.codes replaces NaN entries with -1, so we fill them in with most common value beforehand\n",
    "  df[column] = df[column].fillna(df[column].mode()[0]).cat.codes\n",
    "\n",
    "def find_max_categs(df):\n",
    "  max_unique_vals = 0\n",
    "  for col in df.select_dtypes([\"object\"]):\n",
    "      # We first find a non-NaN entry in the column to examine wether it's a real categorical or not.\n",
    "      # We do this by iterating through entries until we find the row index (i) of a non-NaN entry.\n",
    "      col_index = df.columns.get_loc(col)\n",
    "      i = 0\n",
    "      while pd.isna(df.iloc[i, col_index]) == True:\n",
    "        i += 1\n",
    "      # Now that we know for sure the i is the row index of a non-NaN entry,\n",
    "      # we check if it is a mixture of chars and ints or not by checking if it contains digits.\n",
    "      contains_digits = any(map(str.isdigit, df.iloc[i, col_index]))\n",
    "      if contains_digits == False:\n",
    "        unique_vals = df[col].nunique()\n",
    "        max_unique_vals = unique_vals if unique_vals > max_unique_vals else max_unique_vals\n",
    "  return max_unique_vals\n",
    "\n",
    "def fill_discrete_cols(df, max_vals):\n",
    "  for col in df.columns:\n",
    "    if df[col].nunique() <= max_vals:\n",
    "      df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWVOuHp2Dsun"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NVBel_Go97B"
   },
   "source": [
    "### Processing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmoSZgMY3xqG"
   },
   "source": [
    "Now, we will build processing classes, leaving it to the user to decide what processors with which parameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R078F8yow2ra"
   },
   "source": [
    "*NOTE: After checking for outliers, I have decided to avoid the removal of entries, and scale the data standardly.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOaBLUQBnotk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "\n",
    "\n",
    "class Processor:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def process_data(self, data: pd.DataFrame):\n",
    "    return data\n",
    "\n",
    "\n",
    "class CategToNumConverter(Processor):\n",
    "  def process_data(self, data):\n",
    "    # Because this process involves sub-processes that build on top of each other,\n",
    "    # we need to make a copy of the data, to pass between all the steps.\n",
    "    data_wo_categs = data.copy()\n",
    "    # We iterate through all the non-numerical columns and process each differently,\n",
    "    # depending on wether it's a real categorical feature or a mixture of digits and chars.\n",
    "    for col in data_wo_categs.select_dtypes([\"object\"]):\n",
    "      # We first find a non-NaN entry in the column to examine wether it's a real categorical or not.\n",
    "      # We do this by iterating through entries until we find the row index (i) of a non-NaN entry.\n",
    "      col_index = data_wo_categs.columns.get_loc(col)\n",
    "      i = 0\n",
    "      while pd.isna(data_wo_categs.iloc[i, col_index]) == True:\n",
    "        i += 1\n",
    "      # Now that we know for sure the i is the row index of a non-NaN entry,\n",
    "      # we check if it is a mixture of chars and ints or not by checking if it contains digits.\n",
    "      contains_digits = any(map(str.isdigit, data_wo_categs.iloc[i, col_index]))\n",
    "      if contains_digits:\n",
    "        remove_chars_from_col(data_wo_categs, col)\n",
    "      else:\n",
    "        categ_col_to_num(data_wo_categs, col)\n",
    "    return data_wo_categs\n",
    "\n",
    "\n",
    "class NanFiller(Processor):\n",
    "  def __init__(self, filler):\n",
    "    self.filler = filler\n",
    "  def process_data(self, data):\n",
    "    data_wo_nans = data.copy()\n",
    "    # We would like to make sure that categorical and discrete features are only\n",
    "    #filled with the most common value, and not with the mean/median, which\n",
    "    # don't represent those feature accurately. To do that, we'll first use\n",
    "    # the find_max_categs() function in order to find the maximum amount of\n",
    "    # unique values for a categorical or discrete feature in our dataset,\n",
    "    # to be able to identify which columns are categorical/discrete and which\n",
    "    # are not.\n",
    "    max_categs = find_max_categs(data)\n",
    "    # Now, we fill all the categorical/discrete columns with most common value\n",
    "    # using the fill_discrete_cols() function.\n",
    "    fill_discrete_cols(data_wo_nans, max_categs)\n",
    "    # We fill all the rest with a filler of choice, passed via the\n",
    "    # self.filler attribute.\n",
    "    data_wo_nans = data_wo_nans.fillna(method = self.filler)\n",
    "    return data_wo_nans\n",
    "\n",
    "\n",
    "class Scaler(Processor):\n",
    "  def __init__(self, scaling_method):\n",
    "    self.scaler = scaling_method\n",
    "  def process_data(self, data):\n",
    "    # Our default scaler is the standard scaler, unless stated otherwise.\n",
    "    if self.scaler == 'standard':\n",
    "      scaler = StandardScaler()\n",
    "      data_scaled = scaler.fit_transform(data)\n",
    "      return pd.DataFrame(data_scaled)\n",
    "    else:\n",
    "      scaler = MinMaxScaler()\n",
    "      data_scaled = scaler.fit_transform(data)\n",
    "      return pd.DataFrame(data_scaled)\n",
    "\n",
    "\n",
    "class PCAHandler(Processor):\n",
    "  def __init__(self, num_of_components):\n",
    "    self.num_of_components = num_of_components\n",
    "  def process_data(self, data):\n",
    "    pca = PCA(n_components = self.num_of_components, copy = False)\n",
    "    data_post_pca = pca.fit_transform(data)\n",
    "    return pd.DataFrame(data_post_pca)\n",
    "\n",
    "\n",
    "class FeatureSelect(Processor):\n",
    "  def __init__(self, fs_method, num_of_features):\n",
    "    self.fs_method = fs_method\n",
    "    self.num_of_features = num_of_features\n",
    "  def process_data(self, data):\n",
    "    if self.fs_method == 'f_classif':\n",
    "      data_post_fs = SelectKBest(f_classif, self.num_of_features).fit_transform(X, y)\n",
    "      return pd.DataFrame(data_post_fs)\n",
    "    elif self.fs_method == 'chi2':\n",
    "      data_post_fs = SelectKBest(chi2, self.num_of_features).fit_transform(X, y)\n",
    "      return pd.DataFrame(data_post_fs)\n",
    "    else:\n",
    "      data_post_fs = SelectKBest(mutual_info_classif, self.num_of_features).fit_transform(X, y)\n",
    "      return pd.DataFrame(data_post_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Go_qjtTXDKmd"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrrkNjTGCsWO"
   },
   "source": [
    "### Building a processors list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMaoUUj-CeWs"
   },
   "source": [
    "Now, we shall build a processors list according to the user's specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yR-sWnN0CdnA"
   },
   "outputs": [],
   "source": [
    "# We define the basic pre-processors, which are a must, according to the user's\n",
    "# methods of choice. \n",
    "categoric_handler = CategToNumConverter()\n",
    "nan_handler = NanFiller(config.nan_fill_method)\n",
    "scaler = Scaler(config.scaling_method)\n",
    "# We use them to create a basic pre-processors list.\n",
    "processors = [categoric_handler, nan_handler, scaler]\n",
    "# PCA and/or feature selection is up to the user.\n",
    "if config.use_pca:\n",
    "  processors.append(PCAHandler(config.pca_number_of_componnents))\n",
    "if config.use_feature_select:\n",
    "  processors.append(FeatureSelect(config.fs_method, config.fs_number_of_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJUT0B6EDNFD"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hArt1PeWC3IA"
   },
   "source": [
    "### Applying the pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIC8g1jnC8ax"
   },
   "source": [
    "Finally, all we have to to is define a pre-processing function and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HPQRk11RC3jZ",
    "outputId": "35663f88-75c6-4c0d-8797-73aa42047552"
   },
   "outputs": [],
   "source": [
    "def apply_pre_processors(processors, data, config):\n",
    "  # Initially y is set to None, unless the configuration states it's train mode.\n",
    "  y = None\n",
    "  if config.mode == 'train':\n",
    "    y = pd.DataFrame(data['label'])\n",
    "    data = data.drop(columns=['label'])\n",
    "  print(data, '\\n')\n",
    "  # We iterate though all the accumulated processors and apply each on our data.\n",
    "  for proc in processors:\n",
    "    data = proc.process_data(data)\n",
    "    print(data, '\\n')\n",
    "  \n",
    "  return data, y\n",
    "\n",
    "\n",
    "X, y = apply_pre_processors(processors, data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzTDp1UyGpXU"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sz0Q_YVvyZGe"
   },
   "source": [
    "***\n",
    "## Part 3 | **Model Construction**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ud40r35lJt0_"
   },
   "source": [
    "### Building an list of optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CE8UKm-GGowb"
   },
   "source": [
    "For this part, we would like to build a parameter-searchers list accoring to the user's chosen classifiers configuration.  \n",
    "The interesting part is that for each chosen classifier, we make a cross-validation grid searcher.  \n",
    "This tool helps us find the perfect parameters out of a given list of options **using cross-validation**, and not just the training dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rr6oRwy6Wzd6"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def get_clfs_searcher(config):\n",
    "  # We initialize an empty classifiers array, then check for each possible model\n",
    "  # if it was chosen in the configuration file.\n",
    "  searchers = []\n",
    "\n",
    "  if config.use_gnb:\n",
    "    gnb_clf_parameters = {'priors': [[0.5, 0.5], None]}\n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf_searcher = GridSearchCV(gnb_clf, gnb_clf_parameters)\n",
    "    searchers.append(gnb_clf_searcher)\n",
    "\n",
    "  if config.use_knn:\n",
    "    knn_clf_parameters = {'n_neighbors': [1, 5, 10, 15, 20]}\n",
    "    knn_clf = neighbors.KNeighborsClassifier()\n",
    "    knn_clf_searcher = GridSearchCV(knn_clf, knn_clf_parameters)\n",
    "    searchers.append(knn_clf_searcher)\n",
    "\n",
    "  if config.use_log_reg:\n",
    "    log_reg_clf_parameters = {'C': range(0, 11, 2)}\n",
    "    log_reg_clf = LogisticRegression()\n",
    "    log_reg_clf_searcher = GridSearchCV(log_reg_clf, log_reg_clf_parameters)\n",
    "    searchers.append(log_reg_clf_searcher)\n",
    "\n",
    "  if config.use_mlp:\n",
    "    mlp_clf_parameters = {'hidden_layer_sizes': [(100,), (10, 10)],\n",
    "                          'activation': ['logistic', 'relu'],\n",
    "                          'alpha': [0.0001, 0.0005]}\n",
    "    mlp_clf = MLPClassifier()\n",
    "    mlp_clf_searcher = GridSearchCV(mlp_clf, mlp_clf_parameters)\n",
    "    searchers.append(mlp_clf_searcher)\n",
    "\n",
    "  if config.use_svm:\n",
    "    svc_parameters = {'kernel': ['rbf', 'linear'],\n",
    "                      'C': [1, 2],\n",
    "                      'probability': [True]}\n",
    "                      \n",
    "    svc = svm.SVC()\n",
    "    svc_searcher = GridSearchCV(svc, svc_parameters)\n",
    "    searchers.append(svc_searcher)\n",
    "  \n",
    "  return searchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLW10Ac9J0ZA"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4ofg99zJq2a"
   },
   "source": [
    "### Training, optimization, and results export functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USxieY3KNDq5"
   },
   "source": [
    "Now, we will define two function:  \n",
    "1. Receives a classifier's searcher, uses it to finds the classifier's best parameters, and fits it to the data. Returns a classifier.\n",
    "2. Receives a list of searchers, uses the above function to train and adjust all of their classifiers. Returns a list of trained classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmPJHreZYSv-"
   },
   "outputs": [],
   "source": [
    "def train_and_search_clf(searcher, X, y):\n",
    "  searcher.fit(X, y)\n",
    "  print(searcher.best_params_)\n",
    "  clf = searcher.best_estimator_.fit(X, y)\n",
    "  print(\"train precision of {} is {}\".format(type(clf), sklearn.metrics.precision_score(y, clf.predict(X))))\n",
    "  return clf\n",
    "\n",
    "\n",
    "def train_and_search_clfs(srchrs, X, y):\n",
    "  results = []\n",
    "  for srchr in srchrs:\n",
    "      results.append(train_and_search_clf(srchr, X, y))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMkJ7h6jPOkR"
   },
   "source": [
    "Now, we shall define functions that will define and export our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4Ge9vBGZ94H"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "\n",
    "\n",
    "def plot_roc_curve(ax, fpr, tpr, title):\n",
    "    ax.plot(fpr, tpr, color = 'darkorange', label = 'ROC')\n",
    "    ax.plot([0, 1], [0, 1], color = 'red', linestyle = '--')\n",
    "    ax.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def save(results, X, y):\n",
    "  print(\"Saving started.\")\n",
    "  for clf in results:\n",
    "    y_predicted = clf.predict(X)\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 16))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax = fig.add_subplot(221)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(y, y_predicted)\n",
    "    alpha = ['True', 'False']\n",
    "    beta = ['Positive', 'Negative']\n",
    "    cax = ax.matshow(conf_mat, interpolation = 'nearest')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels(['']+alpha)\n",
    "    ax.set_yticklabels(['']+beta)\n",
    "\n",
    "    for (i, j), z in np.ndenumerate(conf_mat):\n",
    "      ax.text(j, i, '{:0.1f}'.format(z), ha = 'center', va = 'center')\n",
    "\n",
    "    kf = KFold(n_splits = 3, random_state = None, shuffle = False)\n",
    "    curr_k = 2\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "        \n",
    "        curr_fpr, curr_tpr, curr_thresholds = roc_curve(y_test, y_prob[:, 1])\n",
    "        ax = fig.add_subplot(2,2,curr_k)\n",
    "        plot_roc_curve(ax, curr_fpr, curr_tpr,\n",
    "                       'ROC Curve for %i-fold CV\\nwith AUC of %.4f' %(curr_k, auc(curr_fpr, curr_tpr)))\n",
    "        \n",
    "        curr_k += 1\n",
    "    \n",
    "    name = config.target_file + str(type(clf)) + \".jpg\"\n",
    "    plt.savefig(name)\n",
    "    print(\"val precision of {} is {}\".format(type(clf), sklearn.metrics.precision_score(y, clf.predict(X))))\n",
    "    pickle.dump(clf, open(config.target_file + str(type(clf)) + \".p\", 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPHBcB7NYXPe"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPqwuPd7YZQo"
   },
   "source": [
    "***\n",
    "## Part 4 | **Model Training Evaluation**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wf033iSgSTJQ"
   },
   "source": [
    "As you probably noticed, we train, evaluate, optimize and export our results in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jjEz7oJm256a",
    "outputId": "ee5c1ac2-8ac6-407d-89ee-4fb5feff35c4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "clfs = get_clfs_searcher(config)\n",
    "res = train_and_search_clfs(clfs, X_train, y_train)\n",
    "\n",
    "save(res, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejPWdyXVY9Cs"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jp2kQz0azNNv"
   },
   "source": [
    "***\n",
    "## Part 5 | **Prediction Output**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1I8WDBFZCES"
   },
   "source": [
    "**Before running this step we have to change the running mode to test in our configurations, and choose a model to use on the test!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o9VcqGIqu9kR",
    "outputId": "612d27f8-a7fa-4e2e-9d43-1e1cb801e8fe"
   },
   "outputs": [],
   "source": [
    "clf = res[1]\n",
    "test_data = pd.read_csv('test_without_target.csv')\n",
    "#config to test mode\n",
    "X, _ = apply_pre_processors(processors, test_data, config)\n",
    "pred = clf.predict_proba(X)\n",
    "\n",
    "pd.DataFrame(pred[:, 1], columns = [\"pred_proba\"]).to_csv(\"311160808.csv\", columns = [\"pred_proba\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8JAMpBDNokBA"
   ],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
